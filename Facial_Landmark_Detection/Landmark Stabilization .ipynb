{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark Stabilization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to stabilize landmark points in a video\n",
    "\n",
    "When you use Dlib’s Facial Landmark Detection on a video, you will notice they jiggle a bit. When the video is obtained under good and consistent lighting conditions, the landmarks tend to be more stable than when the lighting or imaging conditions are bad.\n",
    "\n",
    "The most important reason for this instability is that the landmarks are detected in every frame independently. There is nothing that ties the information in one frame to the information in the next.\n",
    "\n",
    "In this section, we will present a few strategies for stabilizing facial landmark points in videos. Depending on the algorithm we use for stabilizing the points, we will need up to four different pieces of information for stabilization\n",
    "\n",
    "The location of a landmark in the current frame.\n",
    "\n",
    "The location of the same landmark in the previous frame(s).\n",
    "\n",
    "Pixels intensities in a small window in the current frame around the location of the landmark.\n",
    "\n",
    "Pixels intensities in a small window in the previous frame around the location of the landmark.\n",
    "\n",
    "Note: The methods we use are very general and can be applied to tracking points in general and not just facial landmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving average : A simple solution\n",
    "\n",
    "The easiest solution to stabilize the points is to average the point locations over a small time window. This is called a moving average.\n",
    "\n",
    "For example, you can replace the current location of a landmark by the average location of the landmark over the last 5 frames. You could also use a weighted average, where the landmark location in the frame closest to the current frame is weighted more than the frames further away.\n",
    "\n",
    "The basic assumptions behind using moving average to stabilize landmark points are the following\n",
    "\n",
    "The motion of the points are slow compared to the frame rate of the video.\n",
    "\n",
    "The noise in the estimated landmark locations is zero mean. In other words, the process of averaging points over multiple frames will cancel out the noise.\n",
    "\n",
    "When the first assumption is violated, moving average produces points that lag the actual location of the landmarks.\n",
    "\n",
    "The moving averages algorithm is very easy to implement and therefore quite frequently used.\n",
    "\n",
    "### Kalman Filtering\n",
    "\n",
    "Computer Vision is not rocket science! But in Computer Vision, we do use technology that was initially used for tracking missiles and spacecrafts. One such technique is called Kalman Filtering invented by Rudolf E. Kálmán during the height of the cold war.\n",
    "\n",
    "In this approach, a prediction about the location of the landmark points ( or missiles for that matter ) is made based on the current state ( location + velocity ) of the landmark point. Kalman Filtering takes into account the measurement noise and the state is constantly updated based on quality of prediction on the current frame. This produces better results compared to moving averages, but requires more skill and understanding. However, like the moving average method, the pixel values are not used to estimate the location of landmark points.\n",
    "\n",
    "In this course, we will not cover Kalman Filtering because the method we describe next is more appropriate for tracking points on images.\n",
    "\n",
    "### Optical Flow\n",
    "\n",
    "In the methods described so far, only the location and velocity of the landmarks were used for tracking. However, because landmarks are points in an image, it makes sense to use a small patch around a landmarks in one frame to locate it in the next frame.\n",
    "\n",
    "The technique we describe next is called Optical Flow and it uses pixel information for making a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Large Motion\n",
    "\n",
    "The optical flow equation derived so far only holds for differential motion. Which means it will work when the motion is less than a pixel. Fortunately, there is a way to handle it using Image Pyramids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Detection and Tracking\n",
    "\n",
    "We have learned that we can predict the location of landmark points in the current frame given its location in the previous frame using optical flow.\n",
    "\n",
    "We also know the location of landmarks on the current frame calculated by the Facial Landmark Detector.\n",
    "\n",
    "Which of the two is more accurate?\n",
    "\n",
    "Typically, when the motion is small and the appearance of the tracked landmark does not change between frames, tracking does a very good job. However, it is not uncommon for a tracker to lose track of the point completely because of large motion.\n",
    "\n",
    "On the other hand, the landmark detector usually provides a good rough location of the point.\n",
    "\n",
    "Therefore, when the motion of the face is small, we can choose the location predicted by optical flow. On the other hand, if the landmark locations predicted by the detector and the optical flow tracker are not close, we can assume the tracker has lost track and trust the landmark location predicted by the detector.\n",
    "\n",
    "However, if we make this binary decision to choose one prediction over the other, we will see the points jump around in a non-smooth way. So, we need to combine the two predictions in a more intelligent way.\n",
    "\n",
    "Here is the trick to combine the two estimates.\n",
    "\n",
    "Let,\n",
    "\n",
    "p(t)  = Position of a landmark in the current frame.\n",
    "\n",
    "p(t−1)  = Position of the landmark in the previous frame.\n",
    "\n",
    "p0(t)  = Position of the landmark predicted by optical flow in the current frame.\n",
    "\n",
    "Now, we need to combine  p(t)  and  p0(t)  in some ratio. Let us call this ratio  α  , where\n",
    "0≤α≤1\n",
    " \n",
    "We can find the stabilized point  ps(t)  using the following formula\n",
    "\n",
    "ps(t)=(1−α)p(t)+αp0(t)\n",
    " \n",
    "Stare at that equation for a bit and you will notice, when  α=0.5 , the two predictions contribute equally to the final prediction. When  α=0 , we fully trust the position given by the landmark detector -  p(t)  and when  α=1  , fully trust the prediction given by optical flow -  p0(t) .\n",
    "\n",
    "As mentioned above, we also want this  α  to depend on how fast the point is moving. In other words, we want  α  to depend on the distance between the location of the point in the current frame ( i.e.  p(t) ) and the location of the point in the previous frame (  p(t−1) ).\n",
    "\n",
    "Let,\n",
    "\n",
    "d  = Distance between  p(t)  and  p(t−1)  =  ||p(t)−p(t−1)|| \n",
    "The distance  d  can take any value between 0 and infinity ( well, theoretically speaking ). We need to map it between 0 and 1. To do this we define  α  to be the following\n",
    "\n",
    "α=exp(−d^2/σ^2)\n",
    " \n",
    "Looks exotic, but it is a simple way to map  d  to a number between 0 and 1. What about  σ2 ? It is used to normalize  d  because the distance  d  is in pixels. For a high resolution image the  d  will be higher compared to a lower resolution image for the same level of motion. So those numbers need to be scaled. In our implementation,  d  is normalized by  σ  which is proportional to the distance ( in pixels ) between the corners of the eyes. This is a better measure of scale than image resolution because we can have a small face in a high resolution image and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilizing landmarks using OpenCV\n",
    "\n",
    "As we know from previous sections, one way to implement stabilization requires optical flow calculation. Fortunately, OpenCV has a good implementation of Lukas Kanade optical flow described in this section. It can be invoked using calcOpticalFlowPyrLK.\n",
    "\n",
    "`nextPts, status, err    =   cv.calcOpticalFlowPyrLK(    prevImg, nextImg, prevPts, nextPts[, status[, err[, winSize[, maxLevel[, criteria[, flags[, minEigThreshold]]]]]]]  )`\n",
    "\n",
    "Where,\n",
    "\n",
    "`prevImg` - first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.\n",
    "\n",
    "`nextImg` - second input image or pyramid of the same size and the same type as prevImg.\n",
    "\n",
    "`prevPts` - vector of 2D points for which the flow needs to be found; point coordinates must be single-\n",
    "precision floating-point numbers.\n",
    "\n",
    "`nextPts` - output vector of 2D points (with single-precision floating-point coordinates) containing the calculated new positions of input features in the second image; when OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.\n",
    "\n",
    "`status` - output status vector (of unsigned chars); each element of the vector is set to 1 if the flow for the corresponding features has been found, otherwise, it is set to 0.\n",
    "\n",
    "`err` - output vector of errors; each element of the vector is set to an error for the corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't found then the error is not defined (use the status parameter to find such cases).\n",
    "\n",
    "`winSize` - size of the search window at each pyramid level. maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), if set to 1, two levels are used, and so on; if pyramids are passed to input then algorithm will use as many levels as pyramids have but no more than maxLevel.\n",
    "\n",
    "`criteria` - parameter, specifying the termination criteria of the iterative search algorithm (after the specified maximum number of iterations criteria.maxCount or when the search window moves by less than criteria.epsilon.\n",
    "\n",
    "`flags` - operation flags:\n",
    "\n",
    "`OPTFLOW_USE_INITIAL_FLOW` - uses initial estimations, stored in nextPts; if the flag is not set, then prevPts is copied to nextPts and is considered the initial estimate.\n",
    "    \n",
    "`OPTFLOW_LK_GET_MIN_EIGENVALS` - use minimum eigen values as an error measure (see minEigThreshold description); if the flag is not set, then L1 distance between patches around the original and a moved point, divided by number of pixels in a window, is used as a error measure.\n",
    "    \n",
    "`minEigThreshold` - the algorithm calculates the minimum eigen value of a 2x2 normal matrix of optical flow equations (this matrix is called a spatial gradient matrix), divided by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding feature is filtered out and its flow is not processed, so it allows to remove bad points and get a performance boost.\n",
    "\n",
    "As mentioned earlier, optical flow computation requires building Image pyramids for the current frame and the previous frame. `calcOpticalFlowPyrLKdoes` this calculation internally when you pass the previous frame and the current frame. When using optical flow in a video, the image pyramid for the same frame is built twice -- once while doing optical flow calculation for the current frame and the other for the next frame. This double calculation can be avoided by building and storing the image pyramid for every frame and passing it to calcOpticalFlowPyrLK.\n",
    "\n",
    "The most common usage of buildOpticalFlowPyramid is shown below.\n",
    "\n",
    "`retval, pyramid =   cv.buildOpticalFlowPyramid( img, winSize, maxLevel[, pyramid[, withDerivatives[, pyrBorder[, derivBorder[, tryReuseInputImage]]]]]  )`\n",
    "Where,\n",
    "\n",
    "`img` - 8-bit input image.\n",
    "\n",
    "`pyramid` - output pyramid.\n",
    "\n",
    "`winSize` - window size of optical flow algorithm. Must be not less than winSize argument of calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.\n",
    "\n",
    "`maxLevel` - 0-based maximal pyramid level number.\n",
    "\n",
    "`withDerivatives` - set to precompute gradients for the every pyramid level. If pyramid is constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.\n",
    "\n",
    "`pyrBorder` - the border mode for pyramid layers.\n",
    "\n",
    "`derivBorder` - the border mode for gradients.\n",
    "\n",
    "`tryReuseInputImage` - put ROI of input image into the pyramid if possible. You can pass false to force data copying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, dlib\n",
    "import numpy as np\n",
    "import math, sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0,6.0)\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_PATH = \"shape_predictor_68_face_landmarks.dat\"\n",
    "RESIZE_HEIGHT = 480\n",
    "NUM_FRAMES_FOR_FPS = 100\n",
    "SKIP_FRAMES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the intereye distance.\n",
    "def interEyeDistance(predict):\n",
    "  leftEyeLeftCorner = (predict[36].x, predict[36].y)\n",
    "  rightEyeRightCorner = (predict[45].x, predict[45].y)\n",
    "  distance = cv2.norm(np.array(rightEyeRightCorner) - np.array(leftEyeLeftCorner))\n",
    "  distance = int(distance)\n",
    "  return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winName = \"Stabilized facial landmark detector\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoFileName = \"\"\n",
    "\n",
    "# Initializing video capture object.\n",
    "cap = cv2.VideoCapture(videoFileName)\n",
    "\n",
    "if(cap.isOpened()==False):\n",
    "  print(\"Unable to load video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the parameters of Lucas Kanade method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winSize = 101\n",
    "maxLevel = 10\n",
    "fps = 30.0\n",
    "# Grab a frame\n",
    "ret,imPrev = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to grayscale.\n",
    "imGrayPrev = cv2.cvtColor(imPrev, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the size of the image.\n",
    "size = imPrev.shape[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "landmarkDetector = dlib.shape_predictor(PREDICTOR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the parameters\n",
    "points=[]\n",
    "pointsPrev=[]\n",
    "pointsDetectedCur=[]\n",
    "pointsDetectedPrev=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyeDistanceNotCalculated = True\n",
    "eyeDistance = 0\n",
    "isFirstFrame = True\n",
    "# Initial value, actual value calculated after 100 frames\n",
    "fps = 10\n",
    "showStabilized = False\n",
    "count =0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Detection + Tracking for stabilization \n",
    "\n",
    "As discussed above, we detect the landmarks in each frame and also use the Lucas Kanade method to track the points in the current frame w.r.t previous frame. Then we take a weighted average of the two measurements and that is the stabilized landmark point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "  if (count==0):\n",
    "    t = cv2.getTickCount()\n",
    "\n",
    "  # Grab a frame\n",
    "  ret,im = cap.read()\n",
    "  imDlib = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "  # COnverting to grayscale\n",
    "  imGray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "  height = im.shape[0]\n",
    "  IMAGE_RESIZE = float(height)/RESIZE_HEIGHT\n",
    "  # Resize image for faster face detection\n",
    "  imSmall = cv2.resize(im, None, fx=1.0/IMAGE_RESIZE, fy=1.0/IMAGE_RESIZE,interpolation = cv2.INTER_LINEAR)\n",
    "  imSmallDlib = cv2.cvtColor(imSmall, cv2.COLOR_BGR2RGB)\n",
    "  # Skipping the frames for faster processing\n",
    "  if (count % SKIP_FRAMES == 0):\n",
    "    faces = detector(imSmallDlib,0)\n",
    "\n",
    "  # If no face was detected\n",
    "  if len(faces)==0:\n",
    "    print(\"No face detected\")\n",
    "\n",
    "  # If faces are detected, iterate through each image and detect landmark points\n",
    "  else:\n",
    "    for i in range(0,len(faces)):\n",
    "      print(\"face detected\")\n",
    "      # Face detector was found over a smaller image.\n",
    "      # So, we scale face rectangle to correct size.\n",
    "      newRect = dlib.rectangle(int(faces[i].left() * IMAGE_RESIZE),\n",
    "        int(faces[i].top() * IMAGE_RESIZE),\n",
    "        int(faces[i].right() * IMAGE_RESIZE),\n",
    "        int(faces[i].bottom() * IMAGE_RESIZE))\n",
    "      \n",
    "      # Detect landmarks in current frame\n",
    "      landmarks = landmarkDetector(imDlib, newRect).parts()\n",
    "      \n",
    "      # Handling the first frame of video differently,for the first frame copy the current frame points\n",
    "      \n",
    "      if (isFirstFrame==True):\n",
    "        pointsPrev=[]\n",
    "        pointsDetectedPrev = []\n",
    "        [pointsPrev.append((p.x, p.y)) for p in landmarks]\n",
    "        [pointsDetectedPrev.append((p.x, p.y)) for p in landmarks]\n",
    "\n",
    "      # If not the first frame, copy points from previous frame.\n",
    "      else:\n",
    "        pointsPrev=[]\n",
    "        pointsDetectedPrev = []\n",
    "        pointsPrev = points\n",
    "        pointsDetectedPrev = pointsDetectedCur\n",
    "\n",
    "      # pointsDetectedCur stores results returned by the facial landmark detector\n",
    "      # points stores the stabilized landmark points\n",
    "      points = []\n",
    "      pointsDetectedCur = []\n",
    "      [points.append((p.x, p.y)) for p in landmarks]\n",
    "      [pointsDetectedCur.append((p.x, p.y)) for p in landmarks]\n",
    "\n",
    "      # Convert to numpy float array\n",
    "      pointsArr = np.array(points,np.float32)\n",
    "      pointsPrevArr = np.array(pointsPrev,np.float32)\n",
    "\n",
    "      # If eye distance is not calculated before\n",
    "      if eyeDistanceNotCalculated:\n",
    "        eyeDistance = interEyeDistance(landmarks)\n",
    "        print(eyeDistance)\n",
    "        eyeDistanceNotCalculated = False\n",
    "\n",
    "      if eyeDistance > 100:\n",
    "          dotRadius = 3\n",
    "      else:\n",
    "        dotRadius = 2\n",
    "\n",
    "      print(eyeDistance)\n",
    "      sigma = eyeDistance * eyeDistance / 400\n",
    "      s = 2*int(eyeDistance/4)+1\n",
    "\n",
    "      #  Set up optical flow params\n",
    "      lk_params = dict(winSize  = (s, s), maxLevel = 5, criteria = (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 20, 0.03))\n",
    "      # Python Bug. Calculating pyramids and then calculating optical flow results in an error. So directly images are used.\n",
    "      # ret, imGrayPyr= cv2.buildOpticalFlowPyramid(imGray, (winSize,winSize), maxLevel)\n",
    "\n",
    "      pointsArr,status, err = cv2.calcOpticalFlowPyrLK(imGrayPrev,imGray,pointsPrevArr,pointsArr,**lk_params)\n",
    "      \n",
    "\n",
    "      # Converting to float\n",
    "      pointsArrFloat = np.array(pointsArr,np.float32)\n",
    "\n",
    "      # Converting back to list\n",
    "      points = pointsArrFloat.tolist()\n",
    "\n",
    "      # Final landmark points are a weighted average of\n",
    "      # detected landmarks and tracked landmarks\n",
    "      for k in range(0,len(landmarks)):\n",
    "        d = cv2.norm(np.array(pointsDetectedPrev[k]) - np.array(pointsDetectedCur[k]))\n",
    "        alpha = math.exp(-d*d/sigma)\n",
    "        points[k] = (1 - alpha) * np.array(pointsDetectedCur[k]) + alpha * np.array(points[k])\n",
    "\n",
    "      # Drawing over the stabilized landmark points\n",
    "      if showStabilized is True:\n",
    "        for p in points:\n",
    "          cv2.circle(im,(int(p[0]),int(p[1])),dotRadius, (255,0,0),-1)\n",
    "      else:\n",
    "        for p in pointsDetectedCur:\n",
    "          cv2.circle(im,(int(p[0]),int(p[1])),dotRadius, (0,0,255),-1)\n",
    "\n",
    "      isFirstFrame = False\n",
    "      count = count+1\n",
    "\n",
    "      # Calculating the fps value\n",
    "      if ( count == NUM_FRAMES_FOR_FPS):\n",
    "        t = (cv2.getTickCount()-t)/cv2.getTickFrequency()\n",
    "        fps = NUM_FRAMES_FOR_FPS/t\n",
    "        count = 0\n",
    "        isFirstFrame = True\n",
    "\n",
    "      # Display the landmarks points\n",
    "      cv2.putText(im, \"{:.1f}-fps\".format(fps), (50, size[0]-50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 3,cv2.LINE_AA)\n",
    "      cv2.imshow(winName, im)\n",
    "      key = cv2.waitKey(25) & 0xFF\n",
    "\n",
    "      # Use spacebar to toggle between Stabilized and Unstabilized version.\n",
    "      if key==32:\n",
    "        showStabilized = not showStabilized\n",
    "\n",
    "      # Stop the program.\n",
    "      if key==27:\n",
    "        sys.exit()\n",
    "      # Getting ready for next frame\n",
    "      imPrev = im\n",
    "      imGrayPrev = imGray\n",
    "\n",
    "cv2.destroyAllwindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Further readings:\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/Moving_average\n",
    "\n",
    "2. http://www.analog.com/media/en/technical-documentation/dsp-book/dsp_book_Ch15.pdf\n",
    "\n",
    "3. https://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method\n",
    "\n",
    "4. https://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_feature_tracker\n",
    "\n",
    "5. https://en.wikipedia.org/wiki/Kalman_filter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
